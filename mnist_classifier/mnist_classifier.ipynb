{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "armed-utility",
   "metadata": {},
   "source": [
    "# MNIST Classifier\n",
    "\n",
    "* Written for the Manning Live Project - [\"Semi supervised deep learning with gans for melanoma detection\"](https://liveproject.manning.com/project/146/29/semi-supervised-deep-learning-with-gans-for-melanoma-detection)\n",
    "* Uses a Convolutional Neural Network\n",
    "* Topology of the CNN - 2 convolutional layers, 2 pooling layers, one fully connected layer, one dropout layer\n",
    "* Accuracy - 99.3%\n",
    "* Training time - circa 10mins on my local notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulated-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "data_set_path = '../Datasets'\n",
    "\n",
    "# For consistent results\n",
    "torch.manual_seed(12321)\n",
    "\n",
    "# Hyper parameters\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "lr = 0.003\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-carrier",
   "metadata": {},
   "source": [
    "## Download data to Google Colab\n",
    "\n",
    "* Download data from Google Drive to local Google Colab disk\n",
    "* Allows the code to pickup the data as if it is running locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/GDrive')\n",
    "\n",
    "# Adjust data set path to match where the data has been loaded\n",
    "data_set_path = '/GDrive/MyDrive/Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-belgium",
   "metadata": {},
   "source": [
    "## Allow it to run on the GPU\n",
    "\n",
    "* Code below detects if a GPU is available - if it is will run model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oriental-knowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is currently running on the CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   print(\"Notebook is configured to run on the GPU!\")\n",
    "else:\n",
    "   print(\"Notebook is currently running on the CPU.\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-terrorist",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dated-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_size):\n",
    "    # ToTensor - Converts a PIL Image to a tensor\n",
    "    # Normalize - normalizes the data. The first parameter is the desired\n",
    "    # mean. And the second parameter the desired standard deviation\n",
    "    # Both the mean and standard deviation are vectors - each element\n",
    "    # of the vector corresponds to one \"channel\"\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,)),\n",
    "                                    ])\n",
    "\n",
    "    # Dataset is hosted on a private website. And the website is down\n",
    "    # So instead always rely on local copy - downloaded from GitHub mirror https://github.com/mkolod/MNIST\n",
    "    # http://yann.lecun.com/exdb/mnist/\n",
    "    # Images are black and white 28 x 28 pixel images\n",
    "    # 60,000 images in the training set. 10,000 images in the test set\n",
    "    # Images are returned as PIL images\n",
    "    # Train - loads training database\n",
    "\n",
    "    train_set = datasets.MNIST(data_set_path, download=True, train=True, transform=transform)\n",
    "    # Trainset contains targets - a vector of 60,000 values and data [60000, 28, 28]\n",
    "\n",
    "    test_set = datasets.MNIST(data_set_path, download=True, train=False, transform=transform)\n",
    "    # Testset contains targets - a vector of 10,000 values and data [10000, 28, 28]\n",
    "\n",
    "    # Loaders are iterables over datasets\n",
    "    # Batch size is the number of rows per iteration\n",
    "    # The standard gradient descent algorithm looks at the error of all data points, and steps\n",
    "    # based on the derivative of the total error.\n",
    "    # The stochastic gradient descent (SGD) looks at the error of a batch of points and steps\n",
    "    # based on the derivative of that partial error. The batch is chosen at random\n",
    "\n",
    "    # The loaders support SGD by batching and randomising the order of batches\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = data_loader(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-design",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "* Use the negative log likelihood loss function\n",
    "* Loss function is a measure of the error - like mean squared error\n",
    "or hinge loss (used for SVM)\n",
    "* Negative log likelihood needs log probabilities which LogSoftMax serves up\n",
    "* Rather than train our model on a binary classification - right/wrong - instead\n",
    "we are training our model to give accurate probabilities of the chance of success\n",
    "* Cross entropy loss is the same as negative log likelihood, but the soft max layer\n",
    "is taken care of automatically\n",
    "* Confident but wrong predictions are heavily penalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fresh-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function():\n",
    "    return nn.NLLLoss()\n",
    "\n",
    "criterion = loss_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-wayne",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "* 32 - 3 x 3 square convolutions \n",
    "* RELU layer (rectifier linear unit - max(0, x))\n",
    "* 2 x 2 pooling layer\n",
    "* 64 - 3 x 3 square convolutions \n",
    "* RELU layer \n",
    "* 2 x 2 pooling layer\n",
    "* Fully connected linear layer - 1600 nodes\n",
    "* Dropout layer - to help reduce overfitting\n",
    "* Output layer 10 nodes\n",
    "* LogSoftmax layer on top of the output nodes\n",
    "\n",
    "\n",
    "* Softmax converts the output layer into probabilities proportional to the exponent of the input numbers\n",
    "* LogSoftMax is the log of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "diagnostic-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return nn.Sequential(\n",
    "              nn.Conv2d(1, 32, (3, 3)),\n",
    "              # Parameters 32 * 9 = 288\n",
    "              # Output 32 images that are 26 x 26\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d((2, 2)),\n",
    "              # Output 32 images that are 13 x 13\n",
    "\n",
    "              nn.Conv2d(32, 64, (3, 3)),\n",
    "              # Parameters 64 * 9 = 576\n",
    "              # Output 64 images that are 11 x 11\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d((2, 2)),\n",
    "              # Output 64 images that are 5 x 5 = 1600\n",
    "\n",
    "              nn.Flatten(),\n",
    "              nn.Linear(1600, 128),\n",
    "              nn.Dropout(0.5),\n",
    "              nn.Linear(128, 10),\n",
    "              nn.LogSoftmax(dim=1)\n",
    "            )\n",
    "\n",
    "model = create_model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-competition",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "* Validates model agains the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "australian-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-chocolate",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "\n",
    "* Validate after each epoc but does not use the test error to influence training\n",
    "* Return arrays of training loss and validation as each epoc is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compound-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, train_loader, test_loader, lr, epochs, momentum):\n",
    "    # Each iteration of the loader serves up a pair (images, labels)\n",
    "    # The images are [64, 1, 28, 28] and the labels [64]\n",
    "    # The batch size is 64 images and the images are 28 x 28.\n",
    "    losses = []\n",
    "    test_errors = []\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            # As data streams off the loader, push it onto the GPU so the\n",
    "            # calculation happens on the GPU\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # zeros all the gradients of the weights\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # Calculates all the gradients via backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust weighs based on the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        loss = running_loss / len(train_loader)\n",
    "        test_error = validate(model, test_loader)\n",
    "\n",
    "        print(\"\\nEpocs: \", e + 1)\n",
    "        print(\"Loss: \", loss)\n",
    "        print(\"Test error: \", test_error)\n",
    "        losses.append(loss)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    return losses, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-concrete",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epocs:  1\n",
      "Loss:  0.2944848536478976\n",
      "Test error:  0.9778\n"
     ]
    }
   ],
   "source": [
    "losses, test_error = train(model, criterion, train_loader, test_loader, lr, epochs, momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-fundamentals",
   "metadata": {},
   "source": [
    "## Effect of increasing training\n",
    "\n",
    "* Shows the impact of training loss and test error of increasing the number of epocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss_errors(losses, test_errors):\n",
    "    n = len(losses)\n",
    "    x_axis = range(1, n+1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    fig.suptitle('Model performance with increased epocs')\n",
    "\n",
    "    ax1.plot(x_axis, losses, 'o-')\n",
    "    ax1.set_ylabel('Training loss')\n",
    "    ax1.xaxis.set_visible(False)\n",
    "\n",
    "    ax2.plot(x_axis, test_errors, 'o-')\n",
    "    ax2.set_ylabel('Test accuracy')\n",
    "\n",
    "    ax2.set_xlabel('Epocs')\n",
    "    plt.xticks(x_axis)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "show_loss_errors(losses, test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-chancellor",
   "metadata": {},
   "source": [
    "## Overal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best test accuracy:\", max(test_error))\n",
    "print(\"Number of epocs:\", np.argmax(test_error) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-tonight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
